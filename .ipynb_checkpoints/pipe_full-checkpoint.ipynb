{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "958e190e",
   "metadata": {},
   "source": [
    "If you dont have the dataset, run the below code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac22c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d942f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1mW974SwZsSMH-nr89c2Pe9PPHhT1ifDr\n",
      "From (redirected): https://drive.google.com/uc?id=1mW974SwZsSMH-nr89c2Pe9PPHhT1ifDr&confirm=t&uuid=0476676f-93d2-4b12-a819-7937b499ff98\n",
      "To: /Users/christianbutcher/Desktop/spark/notebooks/reviews.zip\n",
      "100%|██████████████████████████████████████| 3.20M/3.20M [00:00<00:00, 13.9MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'reviews.zip'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdown.download(\"https://drive.google.com/uc?id=1mW974SwZsSMH-nr89c2Pe9PPHhT1ifDr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a820dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f569d7a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.239:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c4c5748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.239:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7feef0878310>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1f917f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "import random\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54a0afb",
   "metadata": {},
   "source": [
    "Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "068a2c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---------+--------------------+\n",
      "| app_id|label|review_id|         review_text|\n",
      "+-------+-----+---------+--------------------+\n",
      "|2156300|    1|136759198|The Demo was grea...|\n",
      "|2372320|    0|136758922|First of, this ga...|\n",
      "|1498040|    1|136761203|Пример того, как ...|\n",
      "|1811990|    1|136761840|I have beaten the...|\n",
      "|1811990|    1|136761635|It really is very...|\n",
      "|1782810|    1|136633021|Great for its cur...|\n",
      "|1649740|    1|136629798|THROW YOUR MONEY ...|\n",
      "|1649740|    1|136629381|I forgot I backed...|\n",
      "|1649740|    1|136628148|Firstly, if you'r...|\n",
      "|1649740|    1|136627883|[h1] HUNT THE NIG...|\n",
      "|1798010|    1|136814124|Out of all the “b...|\n",
      "|2273470|    1|136811469|After just doing ...|\n",
      "|2273470|    1|136810289|Well developed de...|\n",
      "|2329130|    1|136812852|Another title pub...|\n",
      "|2329130|    1|136810307|Rewind or Die is ...|\n",
      "|1928420|    0|137490805|No option to chan...|\n",
      "| 986130|    1|137493446|Awesome game! For...|\n",
      "| 986130|    1|137493372|This is a decent ...|\n",
      "| 986130|    1|137493342|This game is amaz...|\n",
      "| 986130|    1|137492723|Just like space s...|\n",
      "+-------+-----+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json('/Users/christianbutcher/Desktop/spark/reviews/*')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf206d1",
   "metadata": {},
   "source": [
    "Clean the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba53e8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropDuplicates(['review_id'])\n",
    "df = df.filter(df['review_text'] != '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed42e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def lower(text):\n",
    "#   return text.lower()\n",
    "#\n",
    "#lower_udf =udf(lower,StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8586a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Remove nonAscii\n",
    "#def strip_non_ascii(data_str):\n",
    "#''' Returns the string without non ASCII characters'''\n",
    "#    stripped = (c for c in data_str if 0 < ord(c) < 127)\n",
    "#    return ''.join(stripped)\n",
    "# setup pyspark udf function\n",
    "#strip_non_ascii_udf = udf(strip_non_ascii, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca2dc56",
   "metadata": {},
   "source": [
    "Create a balanced data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39c84b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=====================================================>  (24 + 1) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0|  499|\n",
      "|    1|  507|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "n = 500\n",
    "seed = 1\n",
    "\n",
    "fractions = df.groupBy(\"label\").count().withColumn(\"required_n\", n/col(\"count\"))\\\n",
    "                .drop(\"count\").rdd.collectAsMap()\n",
    "\n",
    "df_bal = df.stat.sampleBy(\"label\", fractions, seed)\n",
    "df_bal.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da02a2e0",
   "metadata": {},
   "source": [
    "Split data into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "373f5bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:=================================================>      (22 + 2) / 25]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset Count: 298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = df_bal.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d27b49e",
   "metadata": {},
   "source": [
    "Inititalise pipeline stages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3944bc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"review_text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# stop words\n",
    "stops = StopWordsRemover.loadDefaultStopWords('english')\n",
    "stopwordsRemover = StopWordsRemover(inputCol=regexTokenizer.getOutputCol(), outputCol=\"filtered\", \n",
    "                                   stopWords = stops)\n",
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=stopwordsRemover.getOutputCol(), outputCol=\"features\", \n",
    "                               vocabSize=30000, minDF=5)\n",
    "\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "\n",
    "\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2d5165",
   "metadata": {},
   "source": [
    "Put everything together in the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11a0c2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14a156dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d85b264",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91d94c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Run cross-validation, and choose the best set of parameters.\n",
    "model = crossval.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed93839",
   "metadata": {},
   "source": [
    "Obtain predictions for the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcaee192",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6399bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['app_id',\n",
       " 'label',\n",
       " 'review_id',\n",
       " 'review_text',\n",
       " 'words',\n",
       " 'filtered',\n",
       " 'rawFeatures',\n",
       " 'features',\n",
       " 'rawPrediction',\n",
       " 'probability',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04e81b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+----------+\n",
      "|         review_text|label|         probability|prediction|\n",
      "+--------------------+-----+--------------------+----------+\n",
      "|hai so i dont wit...|    1|[0.29622636001772...|       1.0|\n",
      "|Fun game, a worth...|    1|[0.39636672883789...|       1.0|\n",
      "|Cute, simple and ...|    1|[0.25049880010621...|       1.0|\n",
      "|Wow, where did th...|    1|[0.34961248026696...|       1.0|\n",
      "|This game is just...|    0|[0.68285556407288...|       0.0|\n",
      "|Refunded in less ...|    0|[0.97194186498040...|       0.0|\n",
      "|Tried it for an h...|    0|[0.74694255342842...|       0.0|\n",
      "|BattleBlock Theat...|    1|[0.23425697935099...|       1.0|\n",
      "|It gets stale ver...|    0|[0.86507549452499...|       0.0|\n",
      "|       Oh absolutely|    1|[0.44576065370890...|       1.0|\n",
      "+--------------------+-----+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.select('review_text','label','probability','prediction').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afc87cb",
   "metadata": {},
   "source": [
    "Evaluate the model predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9cc16d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7311054671347239"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd86021",
   "metadata": {},
   "source": [
    "Save the model locally to access later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "961ab5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/04 17:28:43 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /private/var/folders/xg/23cfqt_556d2nhkfsg_q2b300000gn/T/blockmgr-fe5561ad-89c9-49cd-9d58-3501971cccfa. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /private/var/folders/xg/23cfqt_556d2nhkfsg_q2b300000gn/T/blockmgr-fe5561ad-89c9-49cd-9d58-3501971cccfa\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:163)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:110)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1206)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1(DiskBlockManager.scala:374)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$doStop$1$adapted(DiskBlockManager.scala:370)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.doStop(DiskBlockManager.scala:370)\n",
      "\tat org.apache.spark.storage.DiskBlockManager.$anonfun$addShutdownHook$2(DiskBlockManager.scala:352)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)\n",
      "Caused by: java.io.IOException: Cannot run program \"rm\": error=0, Failed to exec spawn helper.\n",
      "\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\n",
      "\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:159)\n",
      "\t... 20 more\n",
      "Caused by: java.io.IOException: error=0, Failed to exec spawn helper.\n",
      "\tat java.base/java.lang.ProcessImpl.forkAndExec(Native Method)\n",
      "\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:340)\n",
      "\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:271)\n",
      "\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\n",
      "\t... 22 more\n"
     ]
    }
   ],
   "source": [
    "model.write().overwrite().save('/Users/christianbutcher/Desktop/spark/model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c852f504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
